{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collectible-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import hazm\n",
    "import pandas # library for working with large JSON file\n",
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "path = \"./IR_data_news_5k.json\"\n",
    "\n",
    "\n",
    "\n",
    "data = pandas.read_json(path)\n",
    "data = data.T\n",
    "\n",
    "stopWords = hazm.stopwords_list()\n",
    "normalizer = Normalizer()\n",
    "stemmer = FindStems()\n",
    "tokenizer = Tokenizer()\n",
    "punctuations = ['،', '.', '»', '«', '؟', '(', ')', '/'] #edited from phase 1.1\n",
    "\n",
    "\n",
    "def not_remove(word):\n",
    "    return not ((word in stopWords) or (word in punctuations))\n",
    "\n",
    "\n",
    "\n",
    "preprocessedDocs = []\n",
    "for doc in data.itertuples():\n",
    "    normalized_doc = normalizer.normalize(doc.content)\n",
    "    tokenized_doc = tokenizer.tokenize_words(normalized_doc)\n",
    "    entries = list(filter(not_remove, tokenized_doc))\n",
    "    stemmed = list(map(stemmer.convert_to_stem, entries)) #swap from phase 1.1\n",
    "    \n",
    "    preprocessedDocs.append(stemmed)\n",
    "\n",
    "posDics = []\n",
    "\n",
    "for i, doc in enumerate(preprocessedDocs):\n",
    "    posDics.append(defaultdict(list))\n",
    "    for j, token in enumerate(doc):\n",
    "        posDics[i][token].append(j)\n",
    "\n",
    "\n",
    "\n",
    "bigDic = {}\n",
    "\n",
    "docFreq = [] \n",
    "# is a list to store frequency of each term in each doc,\n",
    "# for ex: docFreq[0] is a dictionary that have the terms of doc id 0 and its frequency in doc 0.\n",
    "\n",
    "for i, doc in enumerate(preprocessedDocs):\n",
    "    docFreq.append(dict())\n",
    "\n",
    "    for j, term in enumerate(doc):\n",
    "        \n",
    "        if term in bigDic:\n",
    "            bigDic[term][0] += 1\n",
    "        else:\n",
    "            bigDic[term] = []\n",
    "            bigDic[term].append(1)\n",
    "            bigDic[term].append(dict())\n",
    "        \n",
    "        if term in docFreq[i]:\n",
    "            docFreq[i][term] += 1\n",
    "        else:\n",
    "            docFreq[i][term] = 1\n",
    "            bigDic[term][1][i] = []\n",
    "            bigDic[term][1][i].append(list())\n",
    "        bigDic[term][1][i][0].append(j) #positions\n",
    "    \n",
    "\n",
    "N = len(preprocessedDocs)    \n",
    "\n",
    "for i, doc in enumerate(preprocessedDocs):\n",
    "    for j, term in enumerate(doc):\n",
    "        tf = 1 + math.log10(docFreq[i][term])\n",
    "        df = len(bigDic[term][1])\n",
    "        idf = math.log10(N/df)\n",
    "        bigDic[term][1][i].append(tf*idf)\n",
    "\n",
    "\n",
    "def sortBasedOnValue(Dic):\n",
    "    keys = list(Dic.keys())\n",
    "    values = list(Dic.values())\n",
    "    sorted_value_index = np.argsort(values)\n",
    "    return {keys[i]: values[i] for i in sorted_value_index}\n",
    "\n",
    "champion_list_param = 50\n",
    "champion_lists= {}\n",
    "\n",
    "for term in bigDic:\n",
    "    old_tmpDic = bigDic[term][1]\n",
    "    keys = list(old_tmpDic.keys())\n",
    "    tmpDic = {keys[i]: old_tmpDic[k][1] for i, k in enumerate(old_tmpDic)}\n",
    "    champion_lists[term] = list(sortBasedOnValue(tmpDic))[-champion_list_param:]\n",
    "    champion_lists[term].reverse()   # for sorting from highest to lowest.\n",
    "\n",
    "# print(champion_lists['پنجعلی'])\n",
    "\n",
    "bigDicSorted = OrderedDict(sorted(bigDic.items()))\n",
    "\n",
    "# pickleFile_write = open('filePickle', 'ab')\n",
    "# pickle.dump(bigDicSorted, pickleFile_write)\t\t\t\t\t\n",
    "# pickleFile_write.close()\n",
    "\n",
    "\n",
    "def intersection(postings1: list, postings2: list):\n",
    "    return list(set(postings1) & set(postings2))\n",
    "\n",
    "def and_not(postings1: list, postings2: list):\n",
    "    return [x for x in postings1 if x not in postings2]\n",
    "\n",
    "def union(postings1: list, postings2: list):\n",
    "    return list(set(postings1) | set(postings2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "generic-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized query: ['مایکل', 'جردن']\tchampion = False\n",
      "1.doc: 4096\tjaccard score: 0.015151515151515152\n",
      "title: ۲۸ آمریکایی در جام یاشاردوغو/جردن باروز هم می‌آید\n",
      "url: https://www.farsnews.ir/news/14001204000444/۲۸-آمریکایی-در-جام-یاشاردوغو-جردن-باروز-هم-می‌آید\n",
      "2.doc: 4973\tjaccard score: 0.014705882352941176\n",
      "title: تیم منتخب هفته قطر بدون ایرانی‌ها/3 بازیکن خطرناک رقیب سپاهان را بشناسید+فیلم\n",
      "url: https://www.farsnews.ir/news/14001119000103/تیم-منتخب-هفته-قطر-بدون-ایرانی‌ها-3-بازیکن-خطرناک-رقیب-سپاهان-را\n",
      "3.doc: 3148\tjaccard score: 0.014705882352941176\n",
      "title: اتومبیل راننده اسطوره جهان به حراج گذاشته می‌شود\n",
      "url: https://www.farsnews.ir/news/14001215000107/اتومبیل-راننده-اسطوره-جهان-به-حراج-گذاشته-می‌شود\n",
      "4.doc: 4770\tjaccard score: 0.013333333333333334\n",
      "title: عامل حادثه عجیب دربی سویا دستگیر شد\n",
      "url: https://www.farsnews.ir/news/14001028000667/عامل-حادثه-عجیب-دربی-سویا-دستگیر-شد\n",
      "5.doc: 3174\tjaccard score: 0.013157894736842105\n",
      "title: کار سخت مدافعان تیم ملی مقابل دومین گلزن برتر لیگ ستارگان در آزادی\n",
      "url: https://www.farsnews.ir/news/14001104000540/کار-سخت-مدافعان-تیم-ملی-مقابل-دومین-گلزن-برتر-لیگ-ستارگان-در-آزادی\n",
      "-------------------------------------------------------------------------------\n",
      "1.doc: 4973\tcosine score: 0.6188396571797675\n",
      "title: تیم منتخب هفته قطر بدون ایرانی‌ها/3 بازیکن خطرناک رقیب سپاهان را بشناسید+فیلم\n",
      "url: https://www.farsnews.ir/news/14001119000103/تیم-منتخب-هفته-قطر-بدون-ایرانی‌ها-3-بازیکن-خطرناک-رقیب-سپاهان-را\n",
      "2.doc: 4096\tcosine score: 0.5864440107968255\n",
      "title: ۲۸ آمریکایی در جام یاشاردوغو/جردن باروز هم می‌آید\n",
      "url: https://www.farsnews.ir/news/14001204000444/۲۸-آمریکایی-در-جام-یاشاردوغو-جردن-باروز-هم-می‌آید\n",
      "3.doc: 3174\tcosine score: 0.5573432376769988\n",
      "title: کار سخت مدافعان تیم ملی مقابل دومین گلزن برتر لیگ ستارگان در آزادی\n",
      "url: https://www.farsnews.ir/news/14001104000540/کار-سخت-مدافعان-تیم-ملی-مقابل-دومین-گلزن-برتر-لیگ-ستارگان-در-آزادی\n",
      "4.doc: 4770\tcosine score: 0.542778643247395\n",
      "title: عامل حادثه عجیب دربی سویا دستگیر شد\n",
      "url: https://www.farsnews.ir/news/14001028000667/عامل-حادثه-عجیب-دربی-سویا-دستگیر-شد\n",
      "5.doc: 4373\tcosine score: 0.5171702165560663\n",
      "title: دومین خط حمله زهردار قطر منتظر سپاهان/رقابت جذاب ابراهیمی، استراماچونی و اسماعیلی+عکس\n",
      "url: https://www.farsnews.ir/news/14001101000113/دومین-خط-حمله-زهردار-قطر-منتظر-سپاهان-رقابت-جذاب-ابراهیمی-استراماچونی\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# query = input(\"your query: \")\n",
    "query = 'مایکل جردن'\n",
    "# query = 'تنویر افکار عمومی'\n",
    "# query = 'باشگاه¬های فوتسال آسیا'\n",
    "normalized_query = normalizer.normalize(query)\n",
    "tokenized_query = tokenizer.tokenize_words(normalized_query)\n",
    "tokenized_query = list(filter(not_remove, tokenized_query))\n",
    "tokenized_query = list(map(stemmer.convert_to_stem, tokenized_query)) #swap from phase 1.1\n",
    "\n",
    "jaccard = True\n",
    "with_champion = False\n",
    "\n",
    "print(\"tokenized query: {}\\tchampion = {}\".format(tokenized_query, with_champion))\n",
    "\n",
    "# pickleFile_read = open('filePickle', 'rb')\t\n",
    "# bigDicSorted = pickle.load(pickleFile_read)\n",
    "\n",
    "\n",
    "if jaccard:\n",
    "    my_query = set(tokenized_query)\n",
    "\n",
    "    jaccard_score = {}\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        similarity = len(intersection) / len(union)\n",
    "        return similarity\n",
    "\n",
    "\n",
    "    \n",
    "    for term in my_query:\n",
    "        if with_champion :\n",
    "            postings = champion_lists[term]\n",
    "        else:\n",
    "            postings = bigDicSorted[term][1]\n",
    "        for doc in postings:\n",
    "            if doc in jaccard_score:\n",
    "                continue\n",
    "            doc_set = set(preprocessedDocs[doc])\n",
    "            jaccard_score[doc] = jaccard_similarity(my_query, doc_set)\n",
    "\n",
    "\n",
    "    jaccard_scoreSorted = sortBasedOnValue(jaccard_score)\n",
    "    jaccard_newList = list(jaccard_scoreSorted.keys())\n",
    "    jaccard_newList.reverse()\n",
    "\n",
    "\n",
    "    for i, k in enumerate(jaccard_newList):\n",
    "        if i > 4:\n",
    "            break\n",
    "        print('{}.doc: {}\\tjaccard score: {}'.format(i+1, k, jaccard_score[k]))\n",
    "        print('title: {}\\nurl: {}'.format(data.iloc[k][\"title\"], data.iloc[k][\"url\"]))\n",
    "\n",
    "print('-------------------------------------------------------------------------------')\n",
    "qFreq = {}\n",
    "for term in tokenized_query:\n",
    "    if term not in qFreq:\n",
    "        qFreq[term] = 1\n",
    "    else:\n",
    "        qFreq[term] += 1\n",
    "\n",
    "score = {}\n",
    "length = {}\n",
    "checkedTerm = set()\n",
    "for term in tokenized_query:\n",
    "    if term in checkedTerm:\n",
    "        continue\n",
    "    checkedTerm.add(term)\n",
    "    if with_champion :\n",
    "        postings = champion_lists[term]\n",
    "    else:\n",
    "        postings = bigDicSorted[term][1]\n",
    "    tf = 1 + math.log10(qFreq[term])\n",
    "    df = len(bigDicSorted[term][1])\n",
    "    idf = math.log10(N/df)\n",
    "    w = tf * idf\n",
    "\n",
    "    for doc in postings:\n",
    "        if doc in score:\n",
    "            score[doc] += w * bigDicSorted[term][1][doc][1]\n",
    "        else:\n",
    "            score[doc] = w * bigDicSorted[term][1][doc][1]\n",
    "        if doc not in length:\n",
    "            length[doc] = 0\n",
    "            \n",
    "for doc in length :\n",
    "    doc_set = set(preprocessedDocs[doc])\n",
    "    for term in doc_set:\n",
    "        length[doc] += bigDicSorted[term][1][doc][1] * bigDicSorted[term][1][doc][1]\n",
    "for doc in score:\n",
    "    score[doc] /= math.sqrt(length[doc])\n",
    "scoreSorted = sortBasedOnValue(score)\n",
    "newList = list(scoreSorted.keys())\n",
    "newList.reverse()\n",
    "\n",
    "for i, k in enumerate(newList):\n",
    "    if i > 4:\n",
    "        break\n",
    "    print('{}.doc: {}\\tcosine score: {}'.format(i+1, k, score[k]))\n",
    "    print('title: {}\\nurl: {}'.format(data.iloc[k][\"title\"], data.iloc[k][\"url\"]))\n",
    "\n",
    "# pickleFile_read.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c25269a4018224bb4e3cb6b79397037e31419b0dedc1b97e47175df2e08dbf7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
