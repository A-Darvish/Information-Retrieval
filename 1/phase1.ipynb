{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hazm\n",
    "import pandas # library for working with large JSON file\n",
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "path = \"./IR_data_news_5k.json\"\n",
    "\n",
    "\n",
    "data = pandas.read_json(path)\n",
    "data = data.T\n",
    "\n",
    "stopWords = hazm.stopwords_list()\n",
    "normalizer = Normalizer()\n",
    "stemmer = FindStems()\n",
    "tokenizer = Tokenizer()\n",
    "punctuations = ['،', '.', '»', '«', '؟', '(', ')', '/'] #edited from phase 1.1\n",
    "\n",
    "\n",
    "def not_remove(word):\n",
    "    return not ((word in stopWords) or (word in punctuations))\n",
    "\n",
    "\n",
    "preprocessedDocs = []\n",
    "for doc in data.itertuples():\n",
    "    normalized_doc = normalizer.normalize(doc.content)\n",
    "    tokenized_doc = tokenizer.tokenize_words(normalized_doc)\n",
    "    entries = list(filter(not_remove, tokenized_doc))\n",
    "    stemmed = list(map(stemmer.convert_to_stem, entries)) #swap from phase 1.1\n",
    "    \n",
    "    preprocessedDocs.append(stemmed)\n",
    "\n",
    "\n",
    "bigDic = {}\n",
    "docFreq = []\n",
    "for i, doc in enumerate(preprocessedDocs):\n",
    "    docFreq.append(dict())\n",
    "\n",
    "    for j, term in enumerate(doc):\n",
    "        \n",
    "        if term in bigDic:\n",
    "            bigDic[term][0] += 1\n",
    "        else:\n",
    "            bigDic[term] = []\n",
    "            bigDic[term].append(1)\n",
    "            bigDic[term].append(dict())\n",
    "        \n",
    "        if term in docFreq[i]:\n",
    "            docFreq[i][term] += 1\n",
    "        else:\n",
    "            docFreq[i][term] = 1\n",
    "            bigDic[term][1][i] = []\n",
    "        bigDic[term][1][i].append(j) #positions\n",
    "\n",
    "bigDicSorted = OrderedDict(sorted(bigDic.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loving-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized query: ['\"', 'سهمیه', 'المپیک', '\"']\n",
      "1.doc: 584\tfreqs: 2\n",
      "title: سهرابیان: قایق‌ها 21 ماه در گمرک اسیر شد/ممکن بود راضی شویم قایق‌ها را در چهارشنبه سوری بسوزانند\n",
      "url: https://www.farsnews.ir/news/14001008000645/سهرابیان-قایق‌ها-2-ماه-در-گمرک-اسیر-شد-ممکن-بود-راضی-شویم-قایق‌ها-را\n",
      "2.doc: 3197\tfreqs: 1\n",
      "title: بازی‌های آسیایی یا مسابقات جهانی؛ کدام رویداد برای کشتی‌گیران مهمتر است؟\n",
      "url: https://www.farsnews.ir/news/14001022000258/بازی‌های-آسیایی-یا-مسابقات-جهانی-کدام-رویداد-برای-کشتی‌گیران-مهمتر-است\n",
      "3.doc: 25\tfreqs: 1\n",
      "title: کنایه سرمربی سابق تیم ملی کشتی آزاد به دبیر/محمدی: یک دیوانهِ عاشق بودم که سرمربی شدم\n",
      "url: https://www.farsnews.ir/news/14001115000229/کنایه-سرمربی-سابق-تیم-ملی-کشتی-آزاد-به-دبیر-محمدی-یک-دیوانهِ-عاشق\n",
      "4.doc: 1494\tfreqs: 1\n",
      "title: تاکید به برگزاری انتخابات تیراندازی بدون اعلام اسامی نامزدها\n",
      "url: https://www.farsnews.ir/news/14001118000459/تاکید-به-برگزاری-انتخابات-تیراندازی-بدون-اعلام-اسامی-نامزدها\n",
      "5.doc: 2252\tfreqs: 1\n",
      "title: نشست خبری جشنواره یکصد برای انتخاب برترین‌های قرن ورزش ایران برگزار شد\n",
      "url: https://www.farsnews.ir/news/14001223000346/نشست-خبری-جشنواره-یکصد-برای-انتخاب-برترین‌های-قرن-ورزش-ایران-برگزار-شد\n"
     ]
    }
   ],
   "source": [
    "def sortBasedOnValue(Dic):\n",
    "    keys = list(Dic.keys())\n",
    "    values = list(Dic.values())\n",
    "    sorted_value_index = np.argsort(values)\n",
    "    return {keys[i]: values[i] for i in sorted_value_index}\n",
    "\n",
    "\n",
    "def intersection(postings1: list, postings2: list):\n",
    "    return list(set(postings1) & set(postings2))\n",
    "\n",
    "def and_not(postings1: list, postings2: list):\n",
    "    return [x for x in postings1 if x not in postings2]\n",
    "\n",
    "def union(postings1: list, postings2: list):\n",
    "    return list(set(postings1) | set(postings2))\n",
    "\n",
    "\n",
    "# query = input(\"your query: \")\n",
    "# query = 'مایکل ! جردن'\n",
    "# query = 'باشگاه¬های فوتسال آسیا'\n",
    "query = '\"سهمیه المپیک\"'\n",
    "normalized_query = normalizer.normalize(query)\n",
    "tokenized_query = tokenizer.tokenize_words(normalized_query)\n",
    "tokenized_query = list(filter(not_remove, tokenized_query))\n",
    "tokenized_query = list(map(stemmer.convert_to_stem, tokenized_query)) #swap from phase 1.1\n",
    " \n",
    "print(\"tokenized query: {}\".format(tokenized_query))\n",
    "\n",
    "postings_tmp = []\n",
    "not_flg = False\n",
    "phrase_flg = False\n",
    "phrase = []\n",
    "query_words = []\n",
    "not_found = False\n",
    "for i, tkn in enumerate(tokenized_query):\n",
    "    if tkn == '!':\n",
    "        not_flg = True\n",
    "    elif not_flg:\n",
    "        postings_tmp = and_not(postings_tmp, bigDicSorted[tkn][1].keys())\n",
    "        not_flg = False\n",
    "    elif tkn == '\"':\n",
    "        phrase_flg = not phrase_flg\n",
    "    elif phrase_flg:\n",
    "        phrase.append(tkn)\n",
    "    else:\n",
    "        if tkn not in bigDic:\n",
    "            not_found = True\n",
    "        else:\n",
    "            query_words.append(tkn)\n",
    "            if len(postings_tmp) == 0:\n",
    "                postings_tmp = union(postings_tmp, bigDicSorted[tkn][1].keys())\n",
    "            else:\n",
    "                postings_tmp = intersection(postings_tmp, bigDicSorted[tkn][1].keys())\n",
    "\n",
    "if not_found:\n",
    "    postings_tmp.clear()\n",
    "    \n",
    "phrase_query_result = []\n",
    "phrase_res_dict = {} #keys: doc id, val : freq\n",
    "if len(phrase) != 0:\n",
    "    postings_lists = [bigDicSorted[term][1] for term in phrase]\n",
    "    doc_ids = set(postings_lists[0].keys())\n",
    "    for postings in postings_lists[1:]:\n",
    "        doc_ids = doc_ids.intersection(postings.keys())\n",
    "    matches = []\n",
    "    for doc_id in doc_ids:\n",
    "        positions = []\n",
    "        for term, postings in zip(phrase, postings_lists):\n",
    "            positions.append(postings[doc_id])\n",
    "        for pos in positions[0]:\n",
    "            if all(pos+i in positions[i] for i in range(1, len(positions))):\n",
    "                matches.append((doc_id, pos))\n",
    "    phrase_res_dict = {} #keys: doc id, val : freq\n",
    "    for match in matches:\n",
    "        doc_id, pos = match\n",
    "        if doc_id in phrase_res_dict:\n",
    "            phrase_res_dict[doc_id] += 1\n",
    "        else:\n",
    "            phrase_res_dict[doc_id] = 1\n",
    "\n",
    "    # sorted_phrase_dict = sortBasedOnValue(phrase_res_dict)\n",
    "    if len(postings_tmp) == 0:\n",
    "        postings_tmp = phrase_res_dict.keys()\n",
    "    else:\n",
    "        postings_tmp = intersection(postings_tmp, phrase_res_dict.keys())\n",
    "    \n",
    "\n",
    "\n",
    "unrelated_docs = []\n",
    "if len(postings_tmp) < 5:\n",
    "        for w in query_words:\n",
    "            unrelated_docs = union(postings_tmp, bigDicSorted[w][1].keys())\n",
    "    \n",
    "\n",
    "def sortResults(postings):\n",
    "    tempDict = {}\n",
    "    for docId in postings:\n",
    "        sum = 0\n",
    "        if docId in phrase_res_dict:\n",
    "            sum+=phrase_res_dict[docId]\n",
    "        for w in query_words:\n",
    "            sum += docFreq[docId][w]\n",
    "        tempDict[docId] = sum\n",
    "    tempDict = sortBasedOnValue(tempDict)\n",
    "    newList = list(tempDict.keys())\n",
    "    newList.reverse()\n",
    "    return newList, tempDict\n",
    "\n",
    "res, res_dict = sortResults(postings_tmp)\n",
    "for i, k in enumerate(res):\n",
    "    if i > 4:\n",
    "        break\n",
    "    print('{}.doc: {}\\tfreqs: {}'.format(i+1, k, res_dict[k]))\n",
    "    print('title: {}\\nurl: {}'.format(data.iloc[k][\"title\"], data.iloc[k][\"url\"]))\n",
    "\n",
    "unrelated_docs_new, unrelated_docs_dict = sortResults(unrelated_docs)\n",
    "for i, doc in enumerate(unrelated_docs_new):\n",
    "    print(\"*******************\"*5)\n",
    "    if i > 4:\n",
    "        break\n",
    "    print('{}.doc: {}\\tfreqs: {}\\t(this doc is not completely related)'.format(i+1, doc, unrelated_docs_dict[doc]))\n",
    "    print('title: {}\\nurl: {}'.format(data.iloc[doc][\"title\"], data.iloc[doc][\"url\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c25269a4018224bb4e3cb6b79397037e31419b0dedc1b97e47175df2e08dbf7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
